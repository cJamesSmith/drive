{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import torch as T\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24,) (4,)\n"
     ]
    }
   ],
   "source": [
    "env_name = \"BipedalWalker-v3\"\n",
    "env = gym.make(env_name)\n",
    "print(env.observation_space.shape, env.action_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 reward: -83.90816557613067\n",
      "Episode 2 reward: -76.44822417025898\n",
      "Episode 3 reward: -104.40185153550236\n",
      "Episode 4 reward: -103.83301595156112\n",
      "Episode 5 reward: -100.77347654678921\n",
      "Episode 6 reward: -102.69136026430813\n"
     ]
    }
   ],
   "source": [
    "TEST_EPISODE_LENGTH = 6\n",
    "for i in range(TEST_EPISODE_LENGTH):\n",
    "    env.reset()\n",
    "    episode_reward = 0\n",
    "    while True:\n",
    "        _, reward, done, _ = env.step(env.action_space.sample())\n",
    "        episode_reward += reward\n",
    "        if done:\n",
    "            print(f\"Episode {i+1} reward: {episode_reward}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCO(nn.Module):\n",
    "    def __init__(self, env) -> None:\n",
    "        super().__init__()\n",
    "        self.action_dim = env.action_space.shape[0]\n",
    "        self.observation_dim = env.observation_space.shape[0]\n",
    "        self.policy_net = nn.Sequential(\n",
    "            nn.Linear(self.observation_dim, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, self.action_dim),\n",
    "        )\n",
    "        self.inverse_net = nn.Sequential(\n",
    "            nn.Linear(self.observation_dim * 2, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, self.action_dim),\n",
    "        )\n",
    "\n",
    "    def predict(self, observation):\n",
    "        out = self.policy_net(observation)\n",
    "        return out\n",
    "\n",
    "    def predict_inverse(self, observation1, observation2):\n",
    "        observation = T.cat([observation1, observation2], dim=1)\n",
    "        out = self.inverse_net(observation)\n",
    "        return out\n",
    "\n",
    "model = BCO(env).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Dataset_Inverse(Dataset):\n",
    "    def __init__(self, trajs) -> None:\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        for traj in trajs:\n",
    "            for data in traj:\n",
    "                # obs, new_obs, act = dat\n",
    "                self.data.append(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "class Dataset_Policy(Dataset):\n",
    "    def __init__(self, traj) -> None:\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        for data in traj:\n",
    "            # obs, act = dat\n",
    "            self.data.append(data)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train an expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.sac import MlpPolicy\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert = SAC(\n",
    "    policy=MlpPolicy,\n",
    "    env=env,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 960      |\n",
      "|    ep_rew_mean     | 303      |\n",
      "| time/              |          |\n",
      "|    episodes        | 10       |\n",
      "|    fps             | 188      |\n",
      "|    time_elapsed    | 50       |\n",
      "|    total_timesteps | 9596     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.3    |\n",
      "|    critic_loss     | 0.126    |\n",
      "|    ent_coef        | 0.00915  |\n",
      "|    ent_coef_loss   | 0.225    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 409195   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 954      |\n",
      "|    ep_rew_mean     | 304      |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 187      |\n",
      "|    time_elapsed    | 101      |\n",
      "|    total_timesteps | 19083    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.1    |\n",
      "|    critic_loss     | 0.0903   |\n",
      "|    ent_coef        | 0.00885  |\n",
      "|    ent_coef_loss   | -0.133   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 418682   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 952      |\n",
      "|    ep_rew_mean     | 304      |\n",
      "| time/              |          |\n",
      "|    episodes        | 30       |\n",
      "|    fps             | 187      |\n",
      "|    time_elapsed    | 152      |\n",
      "|    total_timesteps | 28558    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.8    |\n",
      "|    critic_loss     | 2.36     |\n",
      "|    ent_coef        | 0.00897  |\n",
      "|    ent_coef_loss   | 2.44     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 428157   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 928      |\n",
      "|    ep_rew_mean     | 293      |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 187      |\n",
      "|    time_elapsed    | 198      |\n",
      "|    total_timesteps | 37136    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.7    |\n",
      "|    critic_loss     | 2.07     |\n",
      "|    ent_coef        | 0.00912  |\n",
      "|    ent_coef_loss   | 0.885    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 436735   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 934      |\n",
      "|    ep_rew_mean     | 295      |\n",
      "| time/              |          |\n",
      "|    episodes        | 50       |\n",
      "|    fps             | 186      |\n",
      "|    time_elapsed    | 249      |\n",
      "|    total_timesteps | 46688    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.1    |\n",
      "|    critic_loss     | 0.145    |\n",
      "|    ent_coef        | 0.00876  |\n",
      "|    ent_coef_loss   | 0.0436   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 446287   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 933      |\n",
      "|    ep_rew_mean     | 297      |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 186      |\n",
      "|    time_elapsed    | 299      |\n",
      "|    total_timesteps | 55995    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.4    |\n",
      "|    critic_loss     | 2.68     |\n",
      "|    ent_coef        | 0.00922  |\n",
      "|    ent_coef_loss   | 0.519    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 455594   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 932      |\n",
      "|    ep_rew_mean     | 298      |\n",
      "| time/              |          |\n",
      "|    episodes        | 70       |\n",
      "|    fps             | 186      |\n",
      "|    time_elapsed    | 349      |\n",
      "|    total_timesteps | 65260    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19      |\n",
      "|    critic_loss     | 0.252    |\n",
      "|    ent_coef        | 0.00901  |\n",
      "|    ent_coef_loss   | 1.5      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 464859   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 930      |\n",
      "|    ep_rew_mean     | 299      |\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 186      |\n",
      "|    time_elapsed    | 398      |\n",
      "|    total_timesteps | 74426    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.8    |\n",
      "|    critic_loss     | 0.0676   |\n",
      "|    ent_coef        | 0.00926  |\n",
      "|    ent_coef_loss   | -1.15    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 474025   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 928      |\n",
      "|    ep_rew_mean     | 300      |\n",
      "| time/              |          |\n",
      "|    episodes        | 90       |\n",
      "|    fps             | 186      |\n",
      "|    time_elapsed    | 447      |\n",
      "|    total_timesteps | 83498    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.6    |\n",
      "|    critic_loss     | 3.03     |\n",
      "|    ent_coef        | 0.00921  |\n",
      "|    ent_coef_loss   | 1.19     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 483097   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 926      |\n",
      "|    ep_rew_mean     | 301      |\n",
      "| time/              |          |\n",
      "|    episodes        | 100      |\n",
      "|    fps             | 186      |\n",
      "|    time_elapsed    | 496      |\n",
      "|    total_timesteps | 92629    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.9    |\n",
      "|    critic_loss     | 0.0813   |\n",
      "|    ent_coef        | 0.00933  |\n",
      "|    ent_coef_loss   | 0.107    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 492228   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.sac.sac.SAC at 0x7f7926d50070>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.gymlibrary.dev/environments/box2d/bipedal_walker/\n",
    "# Good performance: reward > 300 \n",
    "expert.learn(500000, log_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306.72787686274387\n"
     ]
    }
   ],
   "source": [
    "reward, _ = evaluate_policy(expert, env, 10, render=False, warn=False)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert.save('sac-expert-pedelwalker.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert = SAC.load('./sac-expert-pedelwalker.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get expert's traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODE_LENGTH = 500\n",
    "trajs = []\n",
    "while len(trajs) < 800:\n",
    "    traj = []\n",
    "    obs = env.reset()\n",
    "    while True:\n",
    "        action = expert.predict(obs)[0]\n",
    "        new_obs, _, done, _ = env.step(action)\n",
    "        traj.append([obs, new_obs, action])\n",
    "        obs = new_obs\n",
    "        if len(traj) >= EPISODE_LENGTH:\n",
    "            trajs.append(traj)\n",
    "            break\n",
    "        elif done:\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't run these code, cannot reload\n",
    "import pickle\n",
    "trajs_txt = pickle.dumps(trajs)\n",
    "with open('exprt_trajs.pkl', 'wb') as file:\n",
    "    pickle.dump(trajs_txt, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inverse traj demo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n",
      "torch.Size([100, 24]) torch.Size([100, 24])\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100\n",
    "ld_demo = DataLoader(Dataset_Inverse(trajs), batch_size=BATCH_SIZE)\n",
    "print(len(ld_demo))\n",
    "for obs1, obs2, _ in ld_demo:\n",
    "    print(obs1.shape, obs2.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.MSELoss().cuda()\n",
    "optim = T.optim.Adam(model.parameters())\n",
    "\n",
    "EPOCHES = 1000\n",
    "M = 5000\n",
    "\n",
    "EPS = 0.99\n",
    "DECAY = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# trajs_inv = []\n",
    "trajs_inv = deque(maxlen=5000)\n",
    "for e in tqdm(range(EPOCHES)):\n",
    "    \n",
    "    # step1, generate inverse samples\n",
    "    cnt = 0\n",
    "    epn = 0\n",
    "    \n",
    "    rews = 0\n",
    "    \n",
    "#     trajs_inv = []\n",
    "        \n",
    "    while True:\n",
    "        traj = []\n",
    "        rew = 0\n",
    "            \n",
    "        obs = env.reset()\n",
    "        while True:\n",
    "            inp = T.from_numpy(obs).view(((1, )+obs.shape)).float().cuda()\n",
    "            out = model.predict(inp).cpu().detach().numpy()\n",
    "                \n",
    "            if np.random.rand()>=EPS:\n",
    "                act = out[0]\n",
    "            else:\n",
    "                act = env.action_space.sample()\n",
    "                \n",
    "            new_obs, r, done, _ = env.step(act)\n",
    "                \n",
    "            traj.append([obs, new_obs, act])\n",
    "            obs = new_obs\n",
    "            rew += r\n",
    "            \n",
    "            cnt += 1\n",
    "                \n",
    "            if done==True:\n",
    "                rews += rew\n",
    "                trajs_inv.append(traj)\n",
    "                \n",
    "                epn += 1\n",
    "                \n",
    "                break\n",
    "        \n",
    "        if cnt >= M:\n",
    "            break\n",
    "        \n",
    "    rews /= epn\n",
    "    print('Ep %d: reward=%.2f' % (e+1, rews))\n",
    "        \n",
    "    # step2, update inverse model\n",
    "    ld_inv = DataLoader(Dataset_Inverse(trajs_inv), batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    with tqdm(ld_inv) as TQ:\n",
    "        ls_ep = 0\n",
    "        \n",
    "        for obs1, obs2, act in TQ:\n",
    "            out = model.predict_inverse(obs1.float().cuda(), obs2.float().cuda())\n",
    "            ls_bh = loss_func(out, act.cuda())\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            ls_bh.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            ls_bh = ls_bh.cpu().detach().numpy()\n",
    "            TQ.set_postfix(loss_inv='%.3f' % (ls_bh))\n",
    "            ls_ep += ls_bh\n",
    "        \n",
    "        ls_ep /= len(TQ)\n",
    "        print('Ep %d: loss_inv=%.3f' % (e+1, ls_ep))\n",
    "    \n",
    "    # step3, predict inverse action for demo samples\n",
    "    traj_policy = []\n",
    "    \n",
    "    for obs1, obs2, _ in ld_demo:\n",
    "        out = model.predict_inverse(obs1.float().cuda(), obs2.float().cuda())\n",
    "        \n",
    "        obs = obs1.cpu().detach().numpy()\n",
    "        out = out.cpu().detach().numpy()\n",
    "        \n",
    "        for i in range(BATCH_SIZE):\n",
    "            traj_policy.append([obs[i], out[i]])\n",
    "    \n",
    "    # step4, update policy via demo samples\n",
    "    ld_policy = DataLoader(Dataset_Policy(traj_policy), batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    with tqdm(ld_policy) as TQ:\n",
    "        ls_ep = 0\n",
    "        \n",
    "        for obs, act in TQ:\n",
    "            out = model.predict(obs.float().cuda())\n",
    "            ls_bh = loss_func(out, act.cuda())\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            ls_bh.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            ls_bh = ls_bh.cpu().detach().numpy()\n",
    "            TQ.set_postfix(loss_policy='%.3f' % (ls_bh))\n",
    "            ls_ep += ls_bh\n",
    "        \n",
    "        ls_ep /= len(TQ)\n",
    "        print('Ep %d: loss_policy=%.3f' % (e+1, ls_ep))\n",
    "    \n",
    "    # step5, save model\n",
    "    T.save(model.state_dict(), 'Model/model_reacher_%d.pt' % (e+1))\n",
    "    \n",
    "    EPS *= DECAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "163ba63cc8e6fa59655f510cd44ac1cc94d5f983d47df3b2a3d400a8e02e0b0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
