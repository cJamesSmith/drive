{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练专家智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.sac import MlpPolicy\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import gym\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24,) (4,)\n"
     ]
    }
   ],
   "source": [
    "# 创建环境\n",
    "env_name = \"BipedalWalker-v3\"\n",
    "env = gym.make(env_name)\n",
    "print(env.observation_space.shape, env.action_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 reward: -103.30651400911435\n",
      "Episode 2 reward: -106.41460101633675\n",
      "Episode 3 reward: -119.01006671276005\n",
      "Episode 4 reward: -99.51496625272767\n",
      "Episode 5 reward: -120.79805165005507\n",
      "Episode 6 reward: -85.30791510040703\n"
     ]
    }
   ],
   "source": [
    "# 测试环境\n",
    "TEST_EPISODE_LENGTH = 6\n",
    "for i in range(TEST_EPISODE_LENGTH):\n",
    "    env.reset()\n",
    "    episode_reward = 0\n",
    "    while True:\n",
    "        _, reward, done, _ = env.step(env.action_space.sample())\n",
    "        episode_reward += reward\n",
    "        if done:\n",
    "            print(f\"Episode {i+1} reward: {episode_reward}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cxw/pyenvs/robot/lib/python3.8/site-packages/stable_baselines3/common/save_util.py:166: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: Can't get attribute '_make_function' on <module 'cloudpickle.cloudpickle' from '/home/cxw/pyenvs/robot/lib/python3.8/site-packages/cloudpickle/cloudpickle.py'>\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 专家智能体模型\n",
    "# expert = SAC(policy=MlpPolicy, env=env, verbose=1, policy_kwargs={\"net_arch\": [128, 128]})\n",
    "# print(expert.__dict__)\n",
    "# https://www.gymlibrary.dev/environments/box2d/bipedal_walker/\n",
    "# Good performance: reward > 300 \n",
    "# expert.learn(100000, log_interval=10)\n",
    "# reward, _ = evaluate_policy(expert, env, 10, render=False, warn=False)\n",
    "# print(reward)\n",
    "# expert.save('sac-expert-pedelwalker.zip')\n",
    "expert = SAC.load('./sac-expert-pedelwalker.zip')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成专家轨迹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_expert_data(n_episode):\n",
    "    states = []\n",
    "    actions = []\n",
    "    for episode in range(n_episode):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = expert.predict(state)[0]\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            state = next_state\n",
    "    return np.array(states), np.array(actions)\n",
    "\n",
    "\n",
    "env.seed(0)\n",
    "T.manual_seed(0)\n",
    "random.seed(0)\n",
    "n_episode = 50 \n",
    "expert_s, expert_a = sample_expert_data(n_episode)\n",
    "\n",
    "# n_samples = 250000   # 采样30个数据\n",
    "# random_index = random.sample(range(expert_s.shape[0]), n_samples)\n",
    "# expert_s = expert_s[random_index]\n",
    "# expert_a = expert_a[random_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 行为克隆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = T.device(\"cuda\") if T.cuda.is_available() else T.device(\"cpu\")\n",
    "\n",
    "\n",
    "class PolicyNet(T.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            T.nn.Linear(state_dim, hidden_dim),\n",
    "            T.nn.LeakyReLU(),\n",
    "            T.nn.Linear(hidden_dim, hidden_dim),\n",
    "            T.nn.LeakyReLU(),\n",
    "            T.nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BehaviorClone:\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, lr):\n",
    "        self.policy = PolicyNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.optimizer = T.optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.criterion = T.nn.MSELoss()\n",
    "\n",
    "    def learn(self, states, actions):\n",
    "        states = T.tensor(states, dtype=T.float).to(device)\n",
    "        actions = T.tensor(actions, dtype=T.float).to(device)\n",
    "        bc_loss = self.criterion(actions, self.policy(states))\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        bc_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def take_action(self, state):\n",
    "        state = T.tensor([state], dtype=T.float).to(device)\n",
    "        action = self.policy(state)\n",
    "        # action_dist = T.distributions.Categorical(probs)\n",
    "        # action = action_dist.sample()\n",
    "        action = action.cpu().detach().numpy()[0]\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2053526/4171694224.py:30: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  with tqdm(total=n_iterations, desc=\"进度条\") as pbar:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de474fe9d7146d9882b6b893de9363a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "进度条:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 36\u001b[0m\n\u001b[1;32m     32\u001b[0m sample_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(low\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m     33\u001b[0m                                    high\u001b[39m=\u001b[39mexpert_s\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m],\n\u001b[1;32m     34\u001b[0m                                    size\u001b[39m=\u001b[39mbatch_size)\n\u001b[1;32m     35\u001b[0m bc_agent\u001b[39m.\u001b[39mlearn(expert_s[sample_indices], expert_a[sample_indices])\n\u001b[0;32m---> 36\u001b[0m current_return \u001b[39m=\u001b[39m test_agent(bc_agent, env, \u001b[39m5\u001b[39;49m)\n\u001b[1;32m     37\u001b[0m test_returns\u001b[39m.\u001b[39mappend(current_return)\n\u001b[1;32m     38\u001b[0m \u001b[39mif\u001b[39;00m (i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[36], line 9\u001b[0m, in \u001b[0;36mtest_agent\u001b[0;34m(agent, env, n_episode)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m      8\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mtake_action(state)\n\u001b[0;32m----> 9\u001b[0m     next_state, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     10\u001b[0m     state \u001b[39m=\u001b[39m next_state\n\u001b[1;32m     11\u001b[0m     episode_return \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n",
      "File \u001b[0;32m~/pyenvs/robot/lib/python3.8/site-packages/gym/wrappers/time_limit.py:18\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     15\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling reset()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 18\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     19\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     20\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/pyenvs/robot/lib/python3.8/site-packages/gym/envs/box2d/bipedal_walker.py:441\u001b[0m, in \u001b[0;36mBipedalWalker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlidar[i]\u001b[39m.\u001b[39mp1 \u001b[39m=\u001b[39m pos\n\u001b[1;32m    437\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlidar[i]\u001b[39m.\u001b[39mp2 \u001b[39m=\u001b[39m (\n\u001b[1;32m    438\u001b[0m         pos[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m math\u001b[39m.\u001b[39msin(\u001b[39m1.5\u001b[39m \u001b[39m*\u001b[39m i \u001b[39m/\u001b[39m \u001b[39m10.0\u001b[39m) \u001b[39m*\u001b[39m LIDAR_RANGE,\n\u001b[1;32m    439\u001b[0m         pos[\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m math\u001b[39m.\u001b[39mcos(\u001b[39m1.5\u001b[39m \u001b[39m*\u001b[39m i \u001b[39m/\u001b[39m \u001b[39m10.0\u001b[39m) \u001b[39m*\u001b[39m LIDAR_RANGE,\n\u001b[1;32m    440\u001b[0m     )\n\u001b[0;32m--> 441\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mworld\u001b[39m.\u001b[39;49mRayCast(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlidar[i], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlidar[i]\u001b[39m.\u001b[39;49mp1, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlidar[i]\u001b[39m.\u001b[39;49mp2)\n\u001b[1;32m    443\u001b[0m state \u001b[39m=\u001b[39m [\n\u001b[1;32m    444\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhull\u001b[39m.\u001b[39mangle,  \u001b[39m# Normal angles up to 0.5 here, but sure more is possible.\u001b[39;00m\n\u001b[1;32m    445\u001b[0m     \u001b[39m2.0\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhull\u001b[39m.\u001b[39mangularVelocity \u001b[39m/\u001b[39m FPS,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[39m1.0\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlegs[\u001b[39m3\u001b[39m]\u001b[39m.\u001b[39mground_contact \u001b[39melse\u001b[39;00m \u001b[39m0.0\u001b[39m,\n\u001b[1;32m    460\u001b[0m ]\n\u001b[1;32m    461\u001b[0m state \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [l\u001b[39m.\u001b[39mfraction \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlidar]\n",
      "File \u001b[0;32m~/pyenvs/robot/lib/python3.8/site-packages/gym/envs/box2d/bipedal_walker.py:393\u001b[0m, in \u001b[0;36mBipedalWalker.reset.<locals>.LidarCallback.ReportFixture\u001b[0;34m(self, fixture, point, normal, fraction)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mReportFixture\u001b[39m(\u001b[39mself\u001b[39m, fixture, point, normal, fraction):\n\u001b[0;32m--> 393\u001b[0m     \u001b[39mif\u001b[39;00m (fixture\u001b[39m.\u001b[39;49mfilterData\u001b[39m.\u001b[39mcategoryBits \u001b[39m&\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    394\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m    395\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mp2 \u001b[39m=\u001b[39m point\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def test_agent(agent, env, n_episode):\n",
    "    return_list = []\n",
    "    for _ in range(n_episode):\n",
    "        episode_return = 0\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.take_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            state = next_state\n",
    "            episode_return += reward\n",
    "        return_list.append(episode_return)\n",
    "    return np.mean(return_list)\n",
    "\n",
    "\n",
    "env.seed(0)\n",
    "T.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "num_episodes = 250\n",
    "hidden_dim = 128\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "lr = 1e-3\n",
    "bc_agent = BehaviorClone(state_dim, hidden_dim, action_dim, lr)\n",
    "n_iterations = 1000\n",
    "batch_size = 64\n",
    "test_returns = []\n",
    "\n",
    "with tqdm(total=n_iterations, desc=\"进度条\") as pbar:\n",
    "    for i in range(n_iterations):\n",
    "        sample_indices = np.random.randint(low=0,\n",
    "                                           high=expert_s.shape[0],\n",
    "                                           size=batch_size)\n",
    "        bc_agent.learn(expert_s[sample_indices], expert_a[sample_indices])\n",
    "        current_return = test_agent(bc_agent, env, 5)\n",
    "        test_returns.append(current_return)\n",
    "        if (i + 1) % 10 == 0:\n",
    "            pbar.set_postfix({'return': '%.3f' % np.mean(test_returns[-10:])})\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "163ba63cc8e6fa59655f510cd44ac1cc94d5f983d47df3b2a3d400a8e02e0b0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
